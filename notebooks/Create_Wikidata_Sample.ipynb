{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbcf6b4",
   "metadata": {},
   "source": [
    "# Create Sample Wikidata\n",
    "\n",
    "As of 22 June 2022\n",
    "* Translate Stream Updater JSON to SPARQL UPDATE (INSERT/DELETE DATA) statements\n",
    "** To be used in testing of the stress test infrastructure and process\n",
    "** Saved as sparql-update.txt\n",
    "* Capture the earliest (first) triple for each deleted subject/predicate pair in a new file, deleted-triples.nt\n",
    "* Reverse the INSERT/DELETE sequence to completely restore a Wikidata RDF load\n",
    "* Transform the CSV data used in testing the query analysis infrastructure to N-triples \n",
    "** 3 files were provided with entities from the human, scholarly articles, taxon, gene and film subgraphs\n",
    "* Manually add the results from translating the 3 files above to the deleted-triples.nt file, to create wikidata-subset.nt\n",
    "\n",
    "As of 23 June 2022\n",
    "* Reprocessed Stream Updater JSON to create 1 second batches of INSERTS and DELETEs\n",
    "** If triple is both inserted and removed in the same \"batch\", then it is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157f2cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T16:52:44.033345Z",
     "start_time": "2022-06-27T16:52:44.028945Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c76607",
   "metadata": {},
   "source": [
    "## Process the Steaming Updater JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532e094e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T23:21:53.442509Z",
     "start_time": "2022-06-26T23:21:53.294883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take dump of Stream Updater data and convert to SPARQL INSERT/DELETE statements\n",
    "with open(\"wikidata_update_stream_6k_edits_20220531.ndjson\", \"r\") as update_data:\n",
    "    with open('sparql-update.txt', 'w') as sparql_update:\n",
    "        while True:\n",
    "            line = update_data.readline()  # Read an entry\n",
    "            if not line: \n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            operation = entry[\"operation\"]\n",
    "            if operation == \"reconcile\":   # Reconciles are not relevant at this time\n",
    "                continue\n",
    "            request = ''\n",
    "            if \"rdf_added_data\" in entry.keys():\n",
    "                request += \"INSERT DATA { \" + \\\n",
    "                    entry[\"rdf_added_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip() + \"}\\n\"\n",
    "            if \"rdf_deleted_data\" in entry.keys():\n",
    "                request += \"DELETE DATA { \" + \\\n",
    "                    entry[\"rdf_deleted_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip() + \"}\\n\"\n",
    "            if \"rdf_linked_data\" in entry.keys():\n",
    "                request += \"INSERT DATA { \" + \\\n",
    "                    entry[\"rdf_linked_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip() + \"}\\n\"\n",
    "            # Ignore rdf_unlinked_data\n",
    "            sparql_update.write(request)\n",
    "            # Note that operations with sequence_lengths different than 1 are not treated differently\n",
    "            # All information in the JSON output is written as complete triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3e1119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T16:52:54.201342Z",
     "start_time": "2022-06-27T16:52:54.195788Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Capture all integer and decimal properties to correctly process the JSON\n",
    "integer_predicates = ['<http://wikiba.se/ontology#identifiers>', '<http://wikiba.se/ontology#sitelinks>',\n",
    "                      '<http://wikiba.se/ontology#statements>', '<http://schema.org/version>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P1128>', '<http://www.wikidata.org/prop/direct/P1128>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2635>', '<http://www.wikidata.org/prop/direct/P2635>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P8687>', '<http://www.wikidata.org/prop/direct/P8687>']\n",
    "decimal_predicates = ['<http://wikiba.se/ontology#geoLongitude>', '<http://wikiba.se/ontology#geoLatitude>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2044>', '<http://www.wikidata.org/prop/direct/P2044>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2046>', '<http://www.wikidata.org/prop/direct/P2046>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2047>', '<http://www.wikidata.org/prop/direct/P2047>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2048>', '<http://www.wikidata.org/prop/direct/P2048>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2054>', '<http://www.wikidata.org/prop/direct/P2054>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2067>', '<http://www.wikidata.org/prop/direct/P2067>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2101>', '<http://www.wikidata.org/prop/direct/P2101>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2102>', '<http://www.wikidata.org/prop/direct/P2102>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2107>', '<http://www.wikidata.org/prop/direct/P2107>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2119>', '<http://www.wikidata.org/prop/direct/P2119>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2177>', '<http://www.wikidata.org/prop/direct/P2177>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2405>', '<http://www.wikidata.org/prop/direct/P2405>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2565>', '<http://www.wikidata.org/prop/direct/P2565>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2854>', '<http://www.wikidata.org/prop/direct/P2854>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P3063>', '<http://www.wikidata.org/prop/direct/P3063>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P4250>', '<http://www.wikidata.org/prop/direct/P4250>',\n",
    "                      '<http://www.wikidata.org/prop/statement/P2046>', '<http://www.wikidata.org/prop/direct/P2046>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa84701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T16:53:06.881852Z",
     "start_time": "2022-06-27T16:53:06.779825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add first occurrence of each \"deleted\" subj/predicate triple to an output file\n",
    "# May miss a few, unique triples if the object can be multi-valued\n",
    "triples = dict()\n",
    "with open('sparql-update.txt', 'r') as sparql_update:\n",
    "    while True:\n",
    "        line = sparql_update.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.startswith(\"INSERT \"):   # Only processing DELETED triples\n",
    "            continue\n",
    "        del_data = line.split(\"{\")[1].split(\"}\")[0]\n",
    "        statements = del_data.split(\" . \")\n",
    "        for statement in statements:\n",
    "            subj = statement.split('<')[1].split(\">\")[0]\n",
    "            clauses = statement.split(' ;')\n",
    "            first_clause = True\n",
    "            for clause in clauses:\n",
    "                if clause.endswith(\" .\"):   # Last clause does not match \" . \"\n",
    "                    clause = clause[:-2]    # Just get rid of the period\n",
    "                if '> a ' in clause or clause.startswith(' a '):\n",
    "                    pred = 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'\n",
    "                    obj = clause.split(' a ')[1]\n",
    "                else:\n",
    "                    if first_clause:\n",
    "                        # Format is \"<subj> <pred> obj_as_value_or_IRI\"\n",
    "                        pred = clause.split('<')[2].split('>')[0]   \n",
    "                        obj = clause[(clause.find('>', clause.find('>') + 1) + 2):]   # Find 2nd '>' and +2\n",
    "                    else:\n",
    "                        # Format is \" <pred> obj_as_value_or_IRI\"\n",
    "                        pred = clause.split('<')[1].split('>')[0] \n",
    "                        obj = clause[(clause.index('>') + 2):]\n",
    "                if first_clause:            # Clean up boolean\n",
    "                    first_clause = False\n",
    "                subj_pred = subj + pred\n",
    "                if subj_pred not in triples.keys():\n",
    "                    if f'<{pred}>' in integer_predicates:\n",
    "                        triples[subj_pred] = f'<{subj}> <{pred}> \"{obj}\"^^<http://www.w3.org/2001/XMLSchema#integer> .'\n",
    "                    elif f'<{pred}>' in decimal_predicates:\n",
    "                        triples[subj_pred] = f'<{subj}> <{pred}> \"{obj}\"^^<http://www.w3.org/2001/XMLSchema#decimal> .'\n",
    "                    else:\n",
    "                        if \"@\" in obj and \" , \" in obj:   # Multi-valued string, Take first one\n",
    "                            triples[subj_pred] = f\"<{subj}> <{pred}> {obj.split(' , ')[0]} .\"\n",
    "                        else:\n",
    "                            triples[subj_pred] = f\"<{subj}> <{pred}> {obj} .\"\n",
    "with open(\"deleted_triples.nt\", \"w\") as new:\n",
    "    for key, value in triples.items():\n",
    "        new.write(value + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f096ab6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T15:54:35.521963Z",
     "start_time": "2022-06-27T15:54:35.407299Z"
    }
   },
   "outputs": [],
   "source": [
    "# To account for re-adding any new triples or deleting any inserted ones\n",
    "# When operating with a complete dump of the Wikidata RDF\n",
    "# Process the Updater's INSERTs/DELETEs in reverse order, and also reverse the requests such that INSERTs become DELETEs and vice-versa\n",
    "lifo = queue.LifoQueue()\n",
    "with open('sparql-update.txt', 'r') as sparql_update:\n",
    "    while True:\n",
    "        line = sparql_update.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.startswith(\"DELETE \"):\n",
    "            new_line = line.replace(\"DELETE DATA\", \"INSERT DATA\")\n",
    "        else:\n",
    "            new_line = line.replace(\"INSERT DATA\", \"DELETE DATA\")\n",
    "        lifo.put(new_line)\n",
    "with open(\"restore_wikidata.txt\", \"w\") as new:\n",
    "    while not lifo.empty():\n",
    "        new.write(lifo.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e0b0c",
   "metadata": {},
   "source": [
    "# Process the Sample RDF Used for Query Analysis Code Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97456ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T23:27:47.426289Z",
     "start_time": "2022-06-26T23:27:47.386368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert CSV from the test infrastructure for query analyses, into triples\n",
    "for input_file in ('subgraphs', 'scholarly_articles', 'scholarly_articles_and_authors'):\n",
    "    with open(f'{input_file}.csv', newline='') as csvfile:\n",
    "        with open(f'{input_file}.nt', 'w') as wikidata:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                wikidata.write(f\"{row['subject']} {row['predicate']} {row['object']} .\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4dcfd0",
   "metadata": {},
   "source": [
    "# Batch the INSERTs/DELETEs to Send 1 Update/Sec\n",
    "\n",
    "Also:\n",
    "* Ignore an INSERT that is removed by a DELETE in the same 1 second interval\n",
    "* Ignore a DELETE that has a corresponding INSERT in the same 1 second interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb0ccee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T23:29:25.792506Z",
     "start_time": "2022-06-26T23:29:25.784132Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_triples_remove_overlaps(rdf_data: str, is_add: bool, inserts: dict, deletes: dict):\n",
    "    # Split request and create s-p-o triples\n",
    "    statements = rdf_data.split(\" . \")\n",
    "    for statement in statements:\n",
    "        subj = statement.split('<')[1].split(\">\")[0]\n",
    "        clauses = statement.split(' ; ')\n",
    "        first_clause = True\n",
    "        for clause in clauses:\n",
    "            if clause.endswith(\" .\"):   # Last clause does not match \" . \"\n",
    "                clause = clause[:-2]    # Just get rid of the period\n",
    "            if 'a ' in clause:\n",
    "                pred = '-a'\n",
    "                pred_str = 'a'\n",
    "                obj = clause.split('a ')[1]\n",
    "            else:\n",
    "                if first_clause:\n",
    "                    # Format is \"<subj> <pred> obj_as_value_or_IRI\"\n",
    "                    pred = clause.split('<')[2].split('>')[0]   \n",
    "                    obj = clause[(clause.find('>', clause.find('>') + 1) + 2):]   # Find 2nd '>' and +2\n",
    "                else:\n",
    "                    # Format is \" <pred> obj_as_value_or_IRI\"\n",
    "                    pred = clause.split('<')[1].split('>')[0] \n",
    "                    obj = clause[(clause.index('>') + 2):]\n",
    "                pred_str = f'<{pred}>'\n",
    "            if first_clause:            # Clean up boolean\n",
    "                first_clause = False\n",
    "            subj_pred_obj = subj + pred + obj\n",
    "            if is_add and subj_pred_obj in deletes.keys():\n",
    "                # Check that inserted triple is not previously deleted (e.g., is deleted and quickly reinserted)\n",
    "                # If found, remove it\n",
    "                del deletes[subj_pred_obj]\n",
    "                # Since no longer deleted, no need to re-insert it\n",
    "                continue\n",
    "            elif not is_add and subj_pred_obj in inserts.keys():\n",
    "                # Check that deleted triple is not previously inserted (e.g., is inserted and quickly deleted)\n",
    "                # If found, remove it\n",
    "                del inserts[subj_pred_obj]\n",
    "                # Since no longer inserted, no need to delete it\n",
    "                continue\n",
    "            # Add details to appropriate dictionary since triple is new    \n",
    "            if is_add:\n",
    "                inserts[subj_pred_obj] = f'<{subj}> {pred_str} {obj} .'\n",
    "            else:\n",
    "                deletes[subj_pred_obj] = f'<{subj}> {pred_str} {obj} .'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb0a350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T23:29:43.805577Z",
     "start_time": "2022-06-26T23:29:43.500168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take dump of Stream Updater data and convert batches of 1 second insert/delete updates\n",
    "\n",
    "event_time = \"\"   # Starting time for first Updater event\n",
    "inserts = dict()\n",
    "deletes = dict()\n",
    "\n",
    "with open(\"wikidata_update_stream_6k_edits_20220531.ndjson\", \"r\") as update_data:\n",
    "    with open('sparql-update-batches.txt', 'w') as sparql_update:\n",
    "        while True:\n",
    "            line = update_data.readline()  # Read an entry\n",
    "            if not line: \n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            operation = entry[\"operation\"]\n",
    "            if operation == \"reconcile\":   # Reconciles are not relevant at this time\n",
    "                continue\n",
    "            \n",
    "            # Determine \"batch\"\n",
    "            new_event_time = entry[\"event_time\"]\n",
    "            if new_event_time != event_time:\n",
    "                # Save current \"batch\" if event_time is defined \n",
    "                if event_time:     # Not defined for the first pass \n",
    "                    insert_str = ''\n",
    "                    delete_str = ''\n",
    "                    if len(inserts.keys()) > 0:\n",
    "                        for key, value in inserts.items():\n",
    "                            insert_str += value + ' '\n",
    "                    if len(deletes.keys()) > 0:\n",
    "                        for key, value in deletes.items():\n",
    "                            delete_str += value + ' '\n",
    "                    if insert_str and delete_str:\n",
    "                        sparql_update.write(\"INSERT DATA { \" + insert_str[:-1] + \"} \"\n",
    "                                            + \"DELETE DATA { \" + delete_str[:-1] + \"}\\n\")\n",
    "                    elif insert_str and not delete_str:\n",
    "                        sparql_update.write(\"INSERT DATA { \" + insert_str[:-1] + \"}\\n\")\n",
    "                    elif not insert_str and delete_str:\n",
    "                        sparql_update.write(\"DELETE DATA { \" + delete_str[:-1] + \"}\\n\")                       \n",
    "                # Start new \"batch\"\n",
    "                event_time = new_event_time\n",
    "                inserts = dict()\n",
    "                deletes = dict()\n",
    "                \n",
    "            # Process the changes    \n",
    "            if \"rdf_added_data\" in entry.keys():\n",
    "                split_triples_remove_overlaps(\n",
    "                    entry[\"rdf_added_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip(), \n",
    "                    True, inserts, deletes)\n",
    "            if \"rdf_deleted_data\" in entry.keys():\n",
    "                split_triples_remove_overlaps(\n",
    "                    entry[\"rdf_deleted_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip(), \n",
    "                    False, inserts, deletes)\n",
    "            if \"rdf_linked_data\" in entry.keys():\n",
    "                split_triples_remove_overlaps(\n",
    "                    entry[\"rdf_linked_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip(), \n",
    "                    True, inserts, deletes)\n",
    "            # Ignore rdf_unlinked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f72ddc",
   "metadata": {},
   "source": [
    "# Process TSV files to add test triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb5b707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T16:54:02.277213Z",
     "start_time": "2022-06-27T16:54:02.013902Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "tsv_files = [f for f in listdir(\"./test_triples\") if isfile(join(\"./test_triples\", f))]\n",
    "\n",
    "wikidata = open('new_triples.nt', 'w')  # Reset the file\n",
    "wikidata.close()\n",
    "\n",
    "with open('new_triples.nt', 'a') as wikidata:\n",
    "    for input_file in tsv_files:\n",
    "        with open(f'./test_triples/{input_file}') as infile:\n",
    "            text = infile.read()\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.startswith('?subject'):\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts)>2:\n",
    "                    pred = parts[1]\n",
    "                    if pred in integer_predicates:\n",
    "                        wikidata.write(f'{parts[0]} {parts[1]} \"{parts[2]}\"^^<http://www.w3.org/2001/XMLSchema#integer> .\\n')\n",
    "                    elif pred in decimal_predicates:\n",
    "                        wikidata.write(f'{parts[0]} {parts[1]} \"{parts[2]}\"^^<http://www.w3.org/2001/XMLSchema#decimal> .\\n')\n",
    "                    else:\n",
    "                        wikidata.write(f\"{parts[0]} {parts[1]} {parts[2]} .\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f74b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
