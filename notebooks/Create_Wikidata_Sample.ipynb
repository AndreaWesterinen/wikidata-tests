{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbcf6b4",
   "metadata": {},
   "source": [
    "# Create Sample Wikidata\n",
    "\n",
    "* Translate Stream Updater JSON to SPARQL UPDATE (INSERT/DELETE DATA) statements\n",
    "** To be used in stress testing\n",
    "** Saved as sparql-update.txt\n",
    "* Capture the earliest (first) triple for each deleted subject/predicate pair in a new file, deleted-triples.nt\n",
    "* Create subset of Wikidata using subgraphs-5.csv, a test set used in query analyses\n",
    "** The CSV data is used to create an initial, \"small\" set of triples for Wikidata compliance testing\n",
    "** Saved as query-triples.nt\n",
    "* Manually add the deleted-triples.nt triples to the file, query-triples.nt, to create wikidata-subset.nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157f2cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T20:29:37.668161Z",
     "start_time": "2022-06-13T20:29:37.663658Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532e094e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T20:29:39.362826Z",
     "start_time": "2022-06-13T20:29:39.217251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take dump of Stream Updater data and convert to SPARQL INSERT/DELETE statements\n",
    "with open(\"wikidata_update_stream_6k_edits_20220531.ndjson\", \"r\") as update_data:\n",
    "    with open('sparql-update.txt', 'w') as sparql_update:\n",
    "        while True:\n",
    "            line = update_data.readline()  # Read an entry\n",
    "            if not line: \n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            operation = entry[\"operation\"]\n",
    "            if operation == \"reconcile\":   # Reconciles are not relevant at this time\n",
    "                continue\n",
    "            request = ''\n",
    "            if \"rdf_added_data\" in entry.keys():\n",
    "                request += \"INSERT DATA { \" + \\\n",
    "                    entry[\"rdf_added_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip() + \"}\\n\"\n",
    "            if \"rdf_deleted_data\" in entry.keys():\n",
    "                request += \"DELETE DATA { \" + \\\n",
    "                    entry[\"rdf_deleted_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip() + \"}\\n\"\n",
    "            if \"rdf_linked_data\" in entry.keys():\n",
    "                request += \"INSERT DATA { \" + \\\n",
    "                    entry[\"rdf_linked_data\"][\"data\"].replace(\"\\n\", \" \").replace(\"\\t\", \"\").strip() + \"}\\n\"\n",
    "            # Ignore rdf_unlinked_data\n",
    "            sparql_update.write(request)\n",
    "            # Note that operations with sequence_lengths different than 1 are not treated differently\n",
    "            # All information in the JSON output is written as complete triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aa84701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T01:27:30.523804Z",
     "start_time": "2022-06-14T01:27:30.442930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add first \"deleted\" triples (for each subject/predicate pair) to an output file\n",
    "# May miss a few, unique triples if the object can be multi-valued\n",
    "triples = dict()\n",
    "with open('sparql-update.txt', 'r') as sparql_update:\n",
    "    while True:\n",
    "        line = sparql_update.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.startswith(\"INSERT \"):   # Only processing DELETED triples\n",
    "            continue\n",
    "        del_data = line.split(\"{\")[1].split(\"}\")[0]\n",
    "        statements = del_data.split(\" . \")\n",
    "        for statement in statements:\n",
    "            subj = statement.split('<')[1].split(\">\")[0]\n",
    "            clauses = statement.split(' ; ')\n",
    "            first_clause = True\n",
    "            for clause in clauses:\n",
    "                if clause.endswith(\" .\"):\n",
    "                    clause = clause[:-2]\n",
    "                if ' a ' in clause:\n",
    "                    pred = '-a'\n",
    "                else:\n",
    "                    if first_clause:\n",
    "                        pred = clause.split('<')[2].split('>')[0]    # First clause has full subj-pred-obj\n",
    "                    else:\n",
    "                        pred = clause.split('<')[1].split('>')[0]    # Clause is only pred-obj\n",
    "                if subj+pred not in triples.keys():\n",
    "                    if first_clause:    \n",
    "                        triples[subj+pred] = f\"{clause.strip()} .\"\n",
    "                    else:\n",
    "                        triples[subj+pred] = f\"<{subj}> {clause.strip()} .\"\n",
    "                if first_clause:\n",
    "                    first_clause = False\n",
    "with open(\"deleted-triples.nt\", \"w\") as new:\n",
    "    for key, value in triples.items():\n",
    "        new.write(value + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97456ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T20:29:41.424729Z",
     "start_time": "2022-06-13T20:29:41.400828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert CSV from query analyses work into TTL\n",
    "with open('subgraphs_5.csv', newline='') as csvfile:\n",
    "    with open('query-triples.nt', 'w') as wikidata:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            wikidata.write(f\"{row['subject']} {row['predicate']} {row['object']} .\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f096ab6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T01:55:56.237268Z",
     "start_time": "2022-06-14T01:55:56.135576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the Updater's INSERTs/DELETEs in reverse order, and also reverse the requests \n",
    "# So that INSERTs become DELETEs and vice-versa\n",
    "import queue\n",
    "lifo = queue.LifoQueue()\n",
    "with open('sparql-update.txt', 'r') as sparql_update:\n",
    "    while True:\n",
    "        line = sparql_update.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.startswith(\"DELETE \"):\n",
    "            new_line = line.replace(\"DELETE DATA\", \"INSERT DATA\")\n",
    "        else:\n",
    "            new_line = line.replace(\"INSERT DATA\", \"DELETE DATA\")\n",
    "        lifo.put(new_line)\n",
    "with open(\"restore_wikidata.txt\", \"w\") as new:\n",
    "    while not lifo.empty():\n",
    "        new.write(lifo.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff86c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
